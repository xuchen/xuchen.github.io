// The Token Epoch article content
export const articleContent = `
# The Token Epoch: Rethinking Economics of AI Labor from Man-Days to Mega-Token-Hours

## Measuring Intelligence: A New Unit of Productivity for the AI Age

Author: Xuchen Yao

In this series, we explore how **tokens**—the atomic units of generative AI—are becoming a **new form of labor, currency, and infrastructure**, just like water, electricity, and data before them.

As AI systems take on real work—writing, coding, designing, conversing—we must ask:  
**If AI is doing the work, how do we measure its productivity?**

The answer is taking shape in a new unit: **MTH (Mega-Token-Hour)**—a way to quantify the capacity, throughput, and cost of intelligent machines.

But tokens aren't just abstract math. Each one takes real **money**, **energy**, and **infrastructure** to produce—GPU farms, electricity, bandwidth, and global compute logistics.  
 Just like kilowatt-hours or gigabytes, tokens are becoming a priced, measurable resource—one we can trade, allocate, optimize, and eventually, govern.

From **horsepower** to **man-days**, and now to **Mega-Token-Hours**, the story of human progress is reflected in the units we use to measure it.

Welcome to the **Token Epoch**—where AI labor becomes measurable, monetizable, and massively distributed.

## Chapter 1: From Horsepower to Man-Days - An Industrial Revolution Transformed by Units

### **1.1 Horsepower: The Power of the Steam Age** 

"This 2025 Tesla Model S Plaid has a peak power of 760 Kilo-Newton-Meters per Second."

Hearing such numbers, most people would only perceive a string of cold physical units, unable to visualize how powerful this car actually is.

But say this instead: "This Tesla Model S Plaid has 1,020 horsepower."

Imagine 1,020 horses simultaneously pulling you forward at breakneck speed. Suddenly, it's vivid. Almost thrilling.

This is the magic of units. To help people better understand the power of steam engines, 18th-century engineer James Watt invented the term "horsepower." Simply put, 1 horsepower roughly equals the work done by lifting 75 kilograms one meter in one second.

Karl Benz's first automobile had only 0.9 horsepower, but it opened the curtain on the massive productivity transformation of replacing four hooves with four wheels.

From the first 0.9-horsepower Benz automobile to today's 1,020-horsepower electric vehicles, the leap in power witnesses humanity's industrial miracle of replacing horses with machines.

How powerful is 1,020 horsepower exactly? A simple calculation shows it means lifting 77 tons of weight one meter in one second! In reality, this 2.2-ton car can accelerate from zero to 60 mph in just two seconds.

"Horsepower" as a unit is vivid and memorable, spreading widely. In contrast, another power-related term - torque - wasn't so fortunate. Despite being equally important, it remains difficult to popularize due to complex expressions like "Newton-meters," "kilogram-meters," and "pound-feet."

In the age of industrial thunder, horsepower was more than a unit—it was the heartbeat of progress. From the clatter of textile looms and the roar of drills, to ships slicing through oceans, planes tearing through skies, rockets breaking Earth's grip, and even Armstrong's first step on the Moon—each was powered, measured, and made possible by the raw force of horsepower.

### **1.2 Man-Days: The Workload Unit of the Digital Age** 

Then came a quieter revolution. We invented a new type of factory called the "office," where office workers  wore suits instead of overalls, and wielded keyboards instead of wrenches. Countless office workers sat at computers, buried in processing documents, designing proposals, calculating spreadsheets, writing reports, managing operations. The productivity engine of humanity shifted from loud machines to silent cubicles.

And a new unit emerged: the man-day (or person-day).

Just as the 19th century used horsepower to measure steam engines, the 20th century used man-days to quantify manual labor. How many people for how many days does it take to complete a task? For example, developing a small website might be 3 man-days; preparing a product launch presentation might be 20 man-days or 1 man-month.

However, this unit has an inherent bug: people's working hours are inconsistent. Some work diligently for 8 hours, others slack off for 12 hours, and still others slack during the day and work overtime at night. So how do you define a "productive" day? Often, it comes down to fuzzy estimates and trust—or just wishful thinking.

When engineering volumes are massive, adding more people cannot bring linear efficiency improvements. "The Mythical Man-Month" long ago revealed this unit's bug - just like stuffing 9 pregnant women into a pregnancy project won't make the baby born in 1 month, some tasks simply can't be parallelized.

Back in the day, beasts of burden were replaced by motors. Today's overworked office "workhorses" dream of a similar liberation: that one day they can be completely "free," trading their 9-to-5 grind for either early retirement on the golf course or getting paid to scroll through Instagram at their desks.

That liberation might come from the rise of Artificial General Intelligence (AGI).

In the near future, many cognitive tasks may be handled by AI agents—capable of planning, decision-making, and execution. End-to-end automation, from idea to delivery.

But then we face a new challenge: how do we measure the workload of these AI agents?

Will we say, "GPT worked for 3 human-days"? It sounds silly already.

Clearly, a new epoch needs a new unit.

Not horsepower. Not man-day.

But something else entirely.

The answer might be: Mega-Token-Hours.

### **1.3 MTH: Measuring Efficiency in the AGI Era** 

Mega-Token-Hours may sound like gibberish now, but bear with me:

Mega means one million — like a megabyte (MB).

Token is the unit of information processed by large language models — the input and output of AI.

Hour is 1 hour of time.

So "Mega-Token-Hour" (MTH) can be understood as: how many million Tokens an intelligent agent consumes per hour to complete tasks.

But that's a mouthful. People love snappy terms—so like "kg·m/s" became "horsepower,"

Similarly, future "Mega-Token-Hours" might also be simplified to a new word - perhaps called "MTH", or "Megs".

Imagine future work scenarios: "This 2‑min Instagram Reel cost just 5 MTH."

That means the AI consumed 5 million Tokens in one hour, completing topic planning, script writing, AI voiceover, music editing, subtitle generation, video production, platform publishing, and even user tag analysis and optimal posting time. Cost? Maybe just $10!

Another example: "Creating an online ordering app for this new restaurant requires 20 MTH."

That is, an intelligent agent works continuously for 5 hours, consuming 4 million Tokens per hour, completing everything from interface design, feature development, payment integration, testing and deployment, to advertising copy, image beautification, and even fake positive reviews.

Yesterday's knowledge workers sold "human-days." Tomorrow's AI agents deliver value in MTH.

The boss checks the dashboard and grins:

"Just one MTH? Now that's efficient."

This is the world we're about to enter—one where Tokens, Token-hours, and Tokens-per-second redefine productivity.

Here's a teaser of what's coming:

Tokens are a new kind of resource that consumes traditional resources. Think of it this way: a resource that eats other resources.


Generating an English-language clone of Wikipedia using AI takes about 300 kWh — enough electricity to fully charge a Tesla three times and drive from San Diego to Seattle.


While 1TB of internet data might cost $10, accessing 1 trillion tokens of AI intelligence might cost over $1 million in 2025. That's digital gold—scarce and valuable.


To build a supercompute center running at 1 TT/s (that's 1 trillion tokens per second), you'd need the budget equal to half of the entire U.S. GDP and at least 700 Hoover Dams running 24/7 to power it.


The new business model of AI infrastructure is simple: Buy traditional resources,  sell intelligent resources—Tokens. Like Coca-Cola turns water and syrup into brand value, AI firms buy silicon and electricity, and sell intelligence.


It's the early days of a new industrial revolution—only this time, we're not measuring the flow of electrons. We're measuring the rise of artificial intelligence.

## Chapter 2: Why We Need New Productivity Units in the AI Age

Modern civilization is built on precise measurement.

Just as Prometheus brought fire and James Watt built the steam engine, it's the unassuming meters, dials, and units that truly transformed our lives. In our daily routines, we rely on standardized units to make sense of the invisible:

Electricity is measured in kilowatt-hours (kWh)


Battery capacity in milliamp-hours (mAh)


Internet data in KB / MB / GB / TB / PB


Network speed in MB/s or Gbps


And these units aren't just for understanding—they're deeply tied to money:

A typical household might use 500 kWh/month → $75–$100 in electricity bills


An EV with a 100 kWh battery → $10–$20 to charge at home, or up to $50 at public stations


A 10,000 mAh power bank → $15–$30 retail price, can fully charge an iPhone 2–3 times


Mobile data: 20GB/month → $20


Home broadband: 100 Mbps → $50/month; 1 Gbps → $90/month

So how to measure the cost of Large Language Models (LLMs)? Let's break it down to unit and price:


Resource cost = unit × price.

LLM's basic unit is called Token. A Token might be a word, a part of a word (subword), or even just a single letter—depending on the model's encoding scheme.

### **2.1 What is a Token?** 

In AI, a Token is like the electron of the digital mind—small, essential, and everywhere.

Tokens are the smallest units LLMs understand and process. Unlike human linguistics, which splits by characters or words, Tokenization is designed for efficient computation:

A word like apple is often 1 Token


internationalization may be split into international + ization


Chinese characters like 猫 used to be split into multiple Tokens in earlier models (due to encoding), but newer models handle them as a single Token


Emojis, code snippets, and URLs can each have unique tokenization patterns


Tokens offer more flexibility than characters or words. Think of them like LEGO blocks—fine-grained enough to compose anything from simple objects to entire structures.

For example, with Tokens like:

clock, tower, church, bell, you can form phrases like "church tower," "bell tower," "clock tower"


wine, glass, cup, shot, you can compose "wine glass," "coffee cup," "shot glass"


Or combine unfamiliar scientific terms: "streptococcus", "bacillus", "lactobacillus", "thermophile"—even if you've never seen these words, the model can infer meaning by composition.


Tokens don't have to match human intuition. A single Token might represent an entire word—or sometimes a strange fragment. For example:

An earlier version of ChatGPT once struggled to count how many "r"s were in "strawberry" — because the model treated the whole word as one Token, like a black box. Trying to count the "r"s in this particular Token is like being handed a droplet of water and asked how many molecules are inside.

### **2.2 Why Tokens?** 

Because no matter how complex the input, LLMs process everything as a stream of Tokens.

Just as electricity flows through wires, Tokens flow through neural networks—activating, computing, and outputting.

Tokens are the "granularity" of AI:

Generating 1,000 Tokens requires billions of matrix operations


Processing 10,000 Tokens can consume multiple watt-hours of electricity


A million Tokens in/out = significant cost and latency impact


That's why Tokens are AI's atomic currency:

LLMs charge per Token (e.g., $0.003 per 1K tokens)


Model optimization focuses on Tokens per second (TPS)


Future smart infrastructure will track token throughput, cost per token, and energy per token

### **2.3 Why Are Tokens Still So Unfamiliar?** 

Because there's still no universal standard for them.

Different models and platforms have subtle differences in Token definitions and pricing methods, causing:

Users don't know how much resources they actually spent when switching between different models

Enterprises struggle to accurately budget AI system's operating costs

Markets struggle to transparently and standardly trade Token resources

This chaotic state is like the early industrial revolution when various local units and currencies were mixed, making travel and trade extremely inconvenient. 

Historically, humans have suffered greatly from measurement chaos. For example:

Fahrenheit vs. Celsius (temperature)

Miles vs. Kilometers (distance)

Pounds vs. Kilograms (weight)

MM/DD/YYYY vs. DD/MM/YYYY (date formats)

In 1999, NASA's Mars Climate Orbiter was lost because one team used pound-force, the other used newtons—causing the probe to crash into the Martian atmosphere. That mistake cost $193 million.

Standardized measurement matters.

Lord Kelvin once said:

	When you can measure what you are speaking about, and express it in numbers, you know something about it. But when you cannot measure it… your knowledge is meager and unsatisfactory.

In the AI-powered future, Tokens will be indispensable new commodities like electricity, water, or data —invisible but essential.

The AI era needs unified Token standards as its unit of productivity.

## Chapter 3: Three Pillars of the Token Economy - Capacity, Speed, Price

### **3.1. Capacity Units: The T System** 

Just as storage space has hierarchical units like "Byte / KB / MB / GB / TB," tokens also require their own standardized units of measurement.

| Unit | Symbol | Number of Tokens | Analogous Scenario |
| :---- | :---- | :---- | :---- |
| **Token** | T | 1 | One word |
| **Kilotoken** | kT | 10³ | A short article (≈700 words) |
| **Megatoken** | MT | 10⁶ | A long novel |
| **Gigatoken** | GT | 10⁹ | The entire English Wikipedia (≈3 billion words) |
| **Teratoken** | TT | 10¹² | GPT-3 training data scale (≈45 TB of text) |
| **Petatoken** | PT | 10¹⁵ | Estimated total volume of all human digital text |

To avoid misunderstanding, T has three meanings:

- Physical world: T \\= Tesla (magnetic field unit)  
- Digital world: T \\= Tera (trillion)  
- Intelligent age: T \\= Token

For example: 5TT \\= 5 trillion Tokens.

Examples:

- Ordinary blog post: 5 KT  
- Encyclopedia Britannica: about 53 MT  
- English Wikipedia: about 6.4 GT  
- GPT training data: commonly measured in TT, PT

In the future, enterprises purchasing AI resources will be like buying data plans. For example: a 10 GT package is enough for a small company's customer service and content generation for a month.

**Battery Analogy:** Purchasing Token packages \\= buying "AI batteries," for example, a "Token battery" with 50MT capacity. When calling models, Tokens are consumed and power decreases. Complex tasks might rapidly consume large amounts of Tokens, like playing games drains phone batteries faster; while simple tasks are equivalent to low-power mode, allowing longer use of this "battery."

Unfortunately, Tokens cannot be "pre-charged" like batteries, only calculated in real-time on demand \\- you cannot carry a "50MT power bank" everywhere.

### **3.2. Speed Units: The T/s System** 

Similar to how network bandwidth is measured in units like MB/s, the processing speed of large models also requires standardized units. The following are suggested:

| Unit | Name | Analogous Scenario |
| :---- | :---- | :---- |
| T/s or TPS | Tokens per Second | Human typing speed (≈0.5 T/s) |
| kT/s | Kilotokens per Second | Inference speed of a single GPU (e.g., A100 running a 7B model) |
| MT/s | Megatokens per Second | Moderate datacenters  |
| GT/s | Gigatokens per Second | Supercomputing clusters with trillion-parameter models |
| TT/s | Teratokens per Second | A vision for the future |

Examples:

- A high-end GPU performance of 50 KT/s means it can process 50,000 Tokens per second  
- If a task needs to process 30 MT at 100 KT/s throughput speed, it only takes 300 seconds (5 minutes) to complete  
- A low-end GPU performance of 1.85 KT/s means it can process 1,850 Tokens per second; the same 30 MT task would take 4.5 hours at 1.85 KT/s throughput speed

The importance of throughput speed determines waiting time, just like:

- In the 56K modem days, downloading a DVD \\= 200 hours

- Charging an EV on a 120V home outlet \\= 70 hours; with a 240V Level 2 charger \\= 10 hours, which makes it possible to drive by day, charge by night

- Running a large model on a laptop at 2 T/s \\= eternal wait

If an AI company sells you a 5 GT Token package (super large data) but bandwidth is only 30T/s, then in a month you can only consume 78 MT of Tokens at most. You bought almost unlimited data but can only consume it at turtle-speed.

This actually happens in the real world, like certain cloud storage services giving you 2TB space but rate limiting download speeds.

So when buying AI services in the future, remember to check two numbers: capacity (how big is the Token package) and speed (is T/s fast enough)!

### **3.3. Cost Units: $/MT and kWh/MT, the Price Tag of Intelligence** 

Since Tokens are a resource, they should also come with a price tag and an energy footprint—just like:

Electricity is billed in kWh

Internet is billed in GB

We can define large model usage in terms of cost and power consumption:

* **Price**: $/MT – How many dollars per million tokens  
* **Energy**: kWh/MT or kWh/TT – How many kilowatt-hours per million or trillion tokens

### 📊 Token Economics & Energy Table 

| Metric | Unit | Analogy | Real-World Example |
| :---- | :---- | :---- | :---- |
| **Token Price** | $/MT | Like electricity price ($/kWh) | GPT-4: $2/MT for input and $8/MT for output |
| **Token Energy** | kWh/MT | Like a utility meter reading | GPT-3 training ≈ 1.3 GWh/TT |

### Real-World Examples 

* In early 2025,  
    
  * DeepSeek-chat charged roughly $1/MT  
  * DeepSeek-reasoner charged $2/MT  
  * Estimated power usage ≈ 0.045 kWh/MT


* The English Wikipedia has about 6.4 GT (6.4 billion tokens)  
    
* Generating 1 GT of output:  
    
  * Costs: $1280  
  * Energy: 45 kWh  
  * Equivalent to powering a 100-kWh EV for \\~half a charge  
  * So, generating all of Wikipedia would need power from 3 fully charged EVs


* Generating 1 TT of output:  
    
  * Costs: $1.2 million  
  * Energy: 45,000 kWh  
  * Equivalent to a small county's power usage for one hour

### From Electricity to Intelligence: The New Alchemy 

Tokens may look like just "words," but behind them is a conversion of real money and real electricity.

This **electricity → intelligence** transformation is reminiscent of alchemists' dream of turning lead into gold.

Today's conversion efficiency? We're still at the **steam engine stage of the AI revolution:**

* Watt's steam engine (1780s): \\~3% efficiency  
* Modern internal combustion engines: up to 40%  
* Current AI systems: still far from peak efficiency

What does far from peak efficiency mean? Tokens are about 10,000 times more expensive than data – 

### The Gap \\- 1T Byte of Data: $40 vs. 1T Tokens: $1.2M 

In a world where we casually burn 1 TB of mobile data a month for a few dozen dollars, generating 1 TT of intelligent output costs $1.2 million.

That means we need:

* Orders-of-magnitude improvement in compute efficiency  
* Massive cost reduction

to bring intelligence generation closer to how we treat data consumption today.

This isn't just about AI scaling—it's about powering the **next industrial revolution of intelligence.**

### **3.4. Energy Perspective: Tokens Are Extremely Resource-Consuming** 

To help people understand more intuitively, we compare the newly proposed Token measurement system with traditional resources:

| Resource Type | Unit | Typical Price | Energy Consumption |
| :---- | :---- | :---- | :---- |
| Electricity | kWh | $0.12/kWh | \\- |
| Mobile Data | GB | $10/GB | Minimal |
| Cloud Storage | TB | $5/TB/month | Low |
| AI Tokens | MT | $2/MT | 45 kWh/MT |

But this table hides a fact: Tokens are not "read" or "stored", but "calculated." And this "calculation" process is extremely expensive.

Today, 1TB of data traffic costs only tens of dollars, but 1 Terra Tokens might cost $1-2 million.

**In an era where 1TB of data is dirt cheap, 1TT of intelligence still costs like white powder**.

What we desperately need next are "Token Forges"—massive, scalable compute factories capable of mass-producing intelligence at industrial scale.

NVIDIA's Jensen Huang calls them "AI Factories."  And rightly so—because just like the steam engine powered the textile mills, GPU clusters will power the Token mills.

This "digital gold" characteristic reminds us of 19th-century history when aluminum was more expensive than gold - until electrolysis appeared, Napoleon III used aluminum tableware for distinguished guests while ordinary guests could only use gold tableware.

This is the reality of intelligent society: computing power is the new fuel, Tokens are the new resource.

Humanity is entering an era where data is raw material, computing power is the factory, and Tokens are the product. AI costs determine the scale of the intelligent economy.

## Chapter 4: Why Are We Still in the "2G Era" of AI Compute?

Achieving 1GT/s computing power on a single machine isn't actually difficult. The difficulty is: can we equip everyone with 1GT/s Token speed?

In 2025, most people's experience with large models for writing still remains a visibly slow process: waiting ten-plus seconds for a few thousand words to appear on screen; generating a few seconds of video might take several minutes of waiting.

When will large model output speed become lightning fast?

At the 2025 GPU Technology Conference (GTC), Nvidia's Jensen Huang spoke eloquently about new data center capabilities, with one chart predicting the future trend of "AI new infrastructure":

Note the chart's x-axis: "TPS for 1 User" - the Token speed that can be allocated per user, measured in Tokens Per Second. The y-axis is "TPS/MW" - Token speed supported per megawatt of electricity, representing the energy efficiency level of computing centers.

The highlight of this chart is that top-tier computing power Blackwell series can push single-user Token speed to 500 TPS, or 500 Tokens per second.

Looks pretty good, right? Actually not! The key problem is the small text on the chart: "for 1 User" - this is calculated based on universal adoption.

That means if everyone runs an AI assistant, servers can only allocate a mere 200 T/s Token speed per person.

Achieving single 1GT/s computing power isn't hard; the difficulty is providing everyone with 1GT/s computing power. Even in 2025, Huang's computing chart only provides 200 T/s computing power per person.

Imagine if your 5G phone suddenly downgraded to 2G network - Instagram messages would spin for half a minute, scrolling short videos would become a luxury. This is exactly the reality current AI computing power faces: while laboratories can reach GT/s levels, per-capita computing power remains in the "2G era" of 200T/s.

From 200 T/s to 1 billion/s (GT/s), we're still far from the AI life we imagine of instant novels, real-time video editing, on-demand responses.

This reminds me of 1900s New York: although Edison's Pearl Street Station had been operating for 18 years, ordinary households still mainly used kerosene lamps because electricity was too expensive and scarce.

### **4.1. How Communication Networks Made the Leap - AI Compute Must Do the Same** 

Don't underestimate this "Tokens per Second" unit. It's like your mobile network's download speed — it defines your upper limit of experience.

Remember the leap from 2G to 5G?

In the 2G era, we had just 5 KB/s.
Now with 5G, we can hit 1 Gbps.

This upgrade required massive infrastructure investment — but once built, bandwidth could be delivered through existing cellular and fiber networks.

Inspired by this, we propose a "2T to 5T" framework — modeling AI throughput growth after mobile generations:

In the 2G era, websites took a minute to load.
 In the 3G era, images and social media became viable — but videos still buffered.
 4G powered the short video boom: TikTok, Reels, Snap.
 5G enables 4K livestreams with near-zero latency.

Meanwhile, our "Token Network" — the computational bandwidth of AI — is still in the 2T stage: maxing out at 1,000 tokens per second, which means it takes 10 seconds to write a short paragraph. It's still a semi-manual experience.

### **4.2. From Compute Plants to Compute Towers** 

Every mobile network upgrade required enormous infrastructure buildouts.

Taking China as an example, it currently has:

5G base stations: 2.3 million

4G base stations: 6.0 million

2G/3G base stations: 2.5 million

Likewise, the future of the "Token Network" will require the construction of AI base stations — perhaps what Jensen Huang called "AI Factories," or what we might dub Token Power Plants.

And instead of buying data plans, you'll buy Token Bundles. Instead of broadband speed, you'll care about your Token Bandwidth (T/s).

When we complain in 2025 that "generating a 5-second video takes forever," it's the same as complaining in 2005 that it took 7 minutes to download a 3MB MP3 on a 56K modem.
Today, 5G can download a 4K movie in the same time.

In the 5T Era, the world could transform:

Creative Democratization: Anyone can produce real-time AAA game visuals


Educational Uplift: Every student gets a 1GT/s "Einstein AI tutor"


Healthcare Access: Rural clinics receive AI diagnostics on par with top hospitals


When 1GT/s per person becomes as common as a 5G phone today, AI assistants will truly earn the name: real-time companions.

### **4.3. Crossing the Compute Chasm** 

To move from 200T/s to 1GT/s per person, we must overcome three barriers:

Hardware Barrier: Quantum or optical computing


Energy Barrier: Fusion power or space-based solar


Architecture Barrier: Neuromorphic designs or next-gen memory-compute fusion


Technology advances exponentially — but human adoption is linear. These runaway breakthroughs sprint ahead while real-world rollout still stumbles.

From the vantage point of 2025 — the "2T Era" of AI — today's complaints about slow output may one day sound like the nostalgic screech of dial-up modems: quaint reminders of how far we've come in digital civilization.

## Chapter 5: From Man-Days to MTH - The Rise of AI Labor

From man-days to MTH, the future AI intelligent network is remarkably similar to the restart of the electrical era. Except this time, what's transmitted is no longer electricity, but Tokens themselves.

### **5.1. From Man-Days to Token-Hour: AI Agents Redefine Productivity** 

In traditional engineering, labor costs are typically measured in *man-hours* or *man-days*. This pricing model is common in the outsourcing industry — for example, in the U.S., a software engineer's man-hour ranges from $100 to $300, with top-tier consulting firms charging as high as $900/hour.

Here's a snapshot of typical U.S. professional service rates:

| Profession | Man-Hour Rate | Man-Day (8 hours) | Notes |
| :---- | :---- | :---- | :---- |
| Software Engineer | $120–$900 | $960–$7,200 | $400–$900/hr for enterprise, $120–$250 for SMEs |
| Lawyer | $300–$3,000 | $2,400–$24,000 | Top-tier partners up to $3,000/hr |
| Auto Mechanic | $100–$150 | $800–$1,200 | Most garages start at $100/hr |
| Plumber | $125–$150 | $1,000–$1,200 | $125/hr minimum in many cities |
| Designer | $80–$100 | $640–$800 | Standard range for experienced designers |

Many of these tasks — especially junior-level coding, legal drafting, and initial design work — are rapidly being replaced by AI agents. Even for physical labor like auto or plumbing repairs, robotic assistance is already on the horizon.

This pricing model essentially reflects "the cost of human labor." But the rise of AI agents is quietly changing everything.

So how do we measure the workload of an AI agent?

We propose a new unit: Token-Hour (or Mega-Token-Hour, abbreviated as MTH), to quantify the computational effort and efficiency of AI. In short, it measures how many tokens the AI consumes per hour to complete a task.

Let's compare:

Yes — just one hour and 8 MTH ($8) later, the AI agent completes a task that would have taken a human six days.

It's reminiscent of the 19th-century Luddite movement — when machines made textiles 200x faster than humans, resistance was futile. The difference now? These "machines" can code, argue in court, and design.

As AI agents advance, "1 hour" could shrink to just minutes. This new labor unit — MTH —  could become the new norm, much like how we once used "horsepower" to measure engines. Now we might say, "This task costs 8 MTH."

### **5.2. Building National AI Grids** 

If the industrial era ran on electricity, then the Token Epoch will be powered by tokens.

An AI agent's productivity depends on two things: intelligence and speed.

Intelligence refers to whether the AI can understand and complete human-intended tasks. Speed refers to token throughput — the infrastructure's maximum processing rate.

It's like downloading a DVD on a 56K modem — the content is there, but you simply can't get it fast enough. Only with widespread fiber optic internet did Blu-rays become practical. Throughput determines whether AI can be usable.

Let's look at real-world compute:

According to DeepSeek's benchmark report (we are citing DeepSeek as it's open source and have transparent report on algorithms, compute, and cost):

Each H800 GPU costs about \\~$28,000

Power consumption: 300W

8 GPUs offer inference throughput of \\~14,800 tokens/sec

Power cost: $0.1/kWh

Now let's scale:

Imagine this – To reach 1 TT/s throughput, you'd need:

540 million GPUs

$15.1 trillion USD investment

45,000 kWh per second

162 million kWh per hour

For context, the Hoover Dam, America's largest hydroelectric plant, generates around 2 billion kWh per year — that's only about 230,000 kWh per hour.

So, running this AI center at 1 TT/s would require the output of over 700 Hoover Dams operating simultaneously.

And the U.S. as a whole consumes roughly 11 million MWh per day, or about 458 million kWh per hour. That means this single AI datacenter could use up one third of the country's electricity supply in real time.

No wonder Jensen Huang's presentation stops at "200T/s for 1 user." Compute, power, and national budgets simply can't keep up.

That's why hyperscalers are pivoting to nuclear energy. Sam Altman's Stargate project — a $500 billion investment — isn't just sci-fi fantasy.

### **5.3. How Many Tokens Do We Need? The Meaning of a Tera-Token Throughput** 

How many people can 1 TT/s support?

Take a real example from Manus, where the average AI agent task consumes \\~250,000 tokens (0.25 MT). So 1 TT (1,000,000 MT) could support:

4 million tasks per second

Serving 340 million people (US population) — one complex task every 85 seconds per person

Think about your typical workday: How many such tasks do you do? If GenAI can complete a non-trivial task every 85 seconds, we might finally see humans truly "slacking" at work.

From that angle, a national-scale AGI compute center might indeed support 340 million people working full-time with AI.

If we ever hit 1 TT/s compute — what would society look like?

The Productivity Paradox: GDP statistics become obsolete

Employment Reimagined: 90% of white-collar jobs shift to AI supervision roles

Educational Shift: Skill power replaces degrees as the core metric

Innovation Explosion: Individuals can produce institution-grade breakthroughs

Just as people in 1870 couldn't imagine the world electrified, we can't yet fathom a world built on 1 TT/s compute. But one thing is certain — as Faraday once said when asked what use his new discovery was:

"What use is a newborn baby?"

The Token Economy is still a baby. But one day, it may grow into a giant that reshapes the world.

## Chapter 6: Civilization's Leap into the Age of Tokens 

### 6.1 New Literacy Around Tokens 

Our understanding of tokens is evolving—not just as units of computation, but as the foundation of a new societal infrastructure. Much like how the early days of electricity required people to grasp watts and kilowatt-hours, the age of AI demands a new literacy around tokens. This journey spans six conceptual layers:

1. Tokens as the Atomic Unit of Language Models (T)

Tokens are the smallest data unit processed by large language models, akin to how electrons flow in a circuit. They form the basis of all AI computation.

2. Token Scale and Aggregation (MT, GT)

Individual tokens are tiny, but applications rely on billions. To measure real-world impact, we need units like:

KT (Kilo-Tokens)

MT (Mega-Tokens)

GT (Giga-Tokens)

These help us scale our thinking, similar to how bytes evolve into kilobytes and terabytes in computing.

3. Token Throughput (T/s, KT/s, MT/s, GT/s)

AI doesn't process tokens instantly. Throughput—measured in Tokens per Second (T/s)—determines how quickly a model can respond. It's like bandwidth for intelligence.

4. Token Economics ($/MT)

Tokens are now monetized resources. Different providers charge per thousand or million tokens. Pricing varies by model, resembling how utilities like electricity or cloud storage are tiered and metered.

5. Energy Efficiency of Token Processing (kWh/GT)

Energy usage per token (e.g., kWh/GT) is both an environmental and economic concern. More efficient models lower carbon footprints and operational costs.

6. Token-Driven Productivity (MTH, Mega-Token-Hour)

Ultimately, token value lies in its output. We proposed a new unit: MTH (Mega Token Hour)—the AI equivalent of man-hours. This lets us compare human and machine productivity on common ground. As AI becomes the new workforce, MTH becomes the new measure of work.

### 6.2 From Individuals to Nations: A Token-Based Society 

For Individuals

As AI becomes personal, users may subscribe to monthly token plans tailored to their needs and activity levels:

Basic: 1 MT/month — enough for occasional queries, personal agents, or light productivity tasks.

Pro: 10 MT/month — includes priority processing, faster response rates, and more context depth.

Creator: 100 MT/month — designed for power users like educators, designers, and developers using dedicated models for content generation, automation, and simulation.

In the future, smart homes or personal devices might manage token usage automatically, allocating tokens across appliances or assistants like energy budgets.

For Enterprises

Companies will increasingly integrate tokens into operational planning. Departments could receive AI budgets just like they do for electricity or bandwidth:

Marketing: 50 MT/month for multichannel content generation, campaign A/B testing, and social media automation.

Customer Support: 100 MT/month to support 24/7 intelligent response systems and multilingual chat agents.

R&D: 200 MT/month for data mining, simulation, trend forecasting, and real-time experimentation with synthetic datasets.

Larger enterprises may even build internal token management platforms to forecast, monitor, and reallocate usage dynamically.

Token Markets

Just as energy and data have markets, tokenized AI capacity will be traded dynamically:

Futures and Spot Markets: Companies may buy guaranteed throughput in GT/sec contracts months in advance.

Peak/Off-Peak Pricing: Demand-based pricing may arise, where AI services cost more during high traffic.

Risk Instruments: Token hedging products could stabilize model training budgets or mitigate cost surges during product launches or data spikes.

Token ETFs or Indexes: Institutional investors might bundle model access or provider shares into investment products.

National Policy Implications

Once tokens become infrastructure, governments must ensure equitable and secure access:

Token Carbon Tax: Levied based on underlying compute power and energy source, to reward energy-efficient, green AI systems.

Usage Quotas and Rationing: During global emergencies or blackouts, public utilities and healthcare might receive token priority.

Market Regulations: To prevent speculation-driven hoarding or price manipulation in token marketplaces.

Cross-Border AI Flow Policies: Ensuring sovereignty over domestic data, preventing brain drain of models, and aligning with national interests.

In short, the token economy will touch policy realms ranging from climate to commerce, requiring oversight akin to electricity, bandwidth, or currency.

### 6.3 From Horsepower to MTH: Standardizing the Intelligence Economy 

Every transformative technological era has brought with it a new unit of measure—one that not only describes the mechanics of progress but helps to shape how society organizes labor, resources, and ambition:

Steam Age: Horsepower quantified physical strength

Electric Age: Watt quantified energy transfer

Information Age: Bit quantified digital information

Intelligence Age: Token will quantify cognitive work

As we step deeper into the Intelligence Age, we face a challenge similar to our ancestors during the rise of electricity and computing: how to make the invisible measurable, the abstract tangible. Just as horsepower helped factories estimate productivity, and kilowatt-hours helped cities meter power consumption, MTH (Mega Token Hours) offers us a lens to measure the output of artificial cognition.

Measuring intelligence in MTH enables us to:

Benchmark AI output against traditional human labor in standardized units

Distribute AI capacity equitably across individuals, organizations, and regions

Govern and monitor the ethical and environmental use of compute resources

Forecast infrastructure needs, like cloud provisioning and energy grid alignment

Without a standard like MTH, AI risks remaining an opaque force—powerful but unaccountable. By embracing MTH, we embed AI into the economic system in a way that is visible, manageable, and democratically accessible.

"Using man-days to measure AI productivity is like using candles to measure sunlight."
`;
